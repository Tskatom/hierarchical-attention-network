{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention Neural Network\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to train and evaluate the neural network I presented to you. This notebook has the code for preparing the data, setting up the model objects, and training. If you are looking for the code for these models they are contained in two files in the directory:\n",
    "\n",
    "    hierarchical.py\n",
    "    training.py\n",
    "    \n",
    "I've commented the code in there so hopefully it is clear.\n",
    "\n",
    "I also talk about the penalty I added to the loss function that rewards entropy of predicted probabilities. You can find the explanation after the data preparation section.\n",
    "\n",
    "Enjoy!\n",
    "Kiefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from hierarchical import *\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2930, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_seen</th>\n",
       "      <th>message</th>\n",
       "      <th>empathy</th>\n",
       "      <th>parsed</th>\n",
       "      <th>num_words</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>loving my family sad for his relationship</td>\n",
       "      <td>affectionate</td>\n",
       "      <td>loving my family sad for his relationship</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>angry disappointed hopeless</td>\n",
       "      <td>angry</td>\n",
       "      <td>angry disappointed hopeless</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>like i want to scream and beat the shit out of...</td>\n",
       "      <td>angry</td>\n",
       "      <td>like i want to scream and beat the shit out of...</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im sad and angry</td>\n",
       "      <td>angry</td>\n",
       "      <td>im sad and angry</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i am very angry and very sad</td>\n",
       "      <td>angry</td>\n",
       "      <td>i am very angry and very sad</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_seen                                            message       empathy  \\\n",
       "0         1          loving my family sad for his relationship  affectionate   \n",
       "1         1                        angry disappointed hopeless         angry   \n",
       "2         1  like i want to scream and beat the shit out of...         angry   \n",
       "3         1                                   im sad and angry         angry   \n",
       "4         1                       i am very angry and very sad         angry   \n",
       "\n",
       "                                              parsed  num_words  polarity  \n",
       "0          loving my family sad for his relationship          7       1.0  \n",
       "1                        angry disappointed hopeless          3      -1.0  \n",
       "2  like i want to scream and beat the shit out of...         12      -1.0  \n",
       "3                                   im sad and angry          4      -1.0  \n",
       "4                       i am very angry and very sad          7      -1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_csv('./parsed_labels.csv')\n",
    "# drop some columns that are holdovers from a previous save and not needed:\n",
    "words.drop(['inds','labels'], axis=1, inplace=True)\n",
    "\n",
    "words['parsed'] = words['parsed'].map(lambda x: x.strip())\n",
    "print(words.shape)\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing there are some messages that are considered duplicates. Below you can see a subset of the data where parsed is not unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_seen</th>\n",
       "      <th>message</th>\n",
       "      <th>empathy</th>\n",
       "      <th>parsed</th>\n",
       "      <th>num_words</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>4</td>\n",
       "      <td>i am sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>i am sad</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>27</td>\n",
       "      <td>not good</td>\n",
       "      <td>sad</td>\n",
       "      <td>not good</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>36</td>\n",
       "      <td>terrible</td>\n",
       "      <td>sad</td>\n",
       "      <td>terrible</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>125</td>\n",
       "      <td>not good</td>\n",
       "      <td>sad</td>\n",
       "      <td>not good</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>644</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_seen   message empathy    parsed  num_words  polarity\n",
       "2681         4  i am sad     sad  i am sad          3      -1.0\n",
       "2723        27  not good     sad  not good          2      -1.0\n",
       "2725        36  terrible     sad  terrible          1      -1.0\n",
       "2729       125  not good     sad  not good          2      -1.0\n",
       "2731       644       sad     sad       sad          1      -1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[words.parsed.duplicated()].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should retain the `num_seen` counts of the duplicates (which is actually something I didn't do for the presentation analysis) and then drop the duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed</th>\n",
       "      <th>num_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feel good</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling good</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good but not great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good having fun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good i think</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>great</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>happy</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>happy happy happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>happy joyful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i am happy</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i am sad</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i feel amazing</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i feel good</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i feel happy</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i feel really good</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>im feeling good</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>im feeling pretty happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>im feeling very happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>im feeling well</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>im good</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>im great</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>im happy</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>im pretty alright</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>its good</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>kinda happy</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>meh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mild happy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>not good</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ok not great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>okay but not great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>pretty alright</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pretty good</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pretty good actually</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pretty happy</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>quite satisfied</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>really good</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sad</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>so happy</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>terrible</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>very good</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>worthless</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     parsed  num_seen\n",
       "0                                   1\n",
       "1                   amazing         3\n",
       "2                 feel good        17\n",
       "3              feeling good        69\n",
       "4                      good        28\n",
       "5        good but not great         1\n",
       "6           good having fun         2\n",
       "7              good i think         3\n",
       "8                     great        11\n",
       "9                     happy        28\n",
       "10        happy happy happy         1\n",
       "11             happy joyful         1\n",
       "12               i am happy        11\n",
       "13                 i am sad         4\n",
       "14           i feel amazing        10\n",
       "15              i feel good       213\n",
       "16             i feel happy        57\n",
       "17       i feel really good        10\n",
       "18          im feeling good        94\n",
       "19  im feeling pretty happy         1\n",
       "20    im feeling very happy         1\n",
       "21          im feeling well         7\n",
       "22                  im good       113\n",
       "23                 im great         7\n",
       "24                 im happy        83\n",
       "25        im pretty alright         1\n",
       "26                 its good        30\n",
       "27              kinda happy        13\n",
       "28                      meh         1\n",
       "29               mild happy         1\n",
       "30                 not good       152\n",
       "31             ok not great         1\n",
       "32       okay but not great         1\n",
       "33           pretty alright         1\n",
       "34              pretty good       810\n",
       "35     pretty good actually        19\n",
       "36             pretty happy        56\n",
       "37          quite satisfied         1\n",
       "38              really good        70\n",
       "39                      sad       645\n",
       "40                 so happy         8\n",
       "41                 terrible        36\n",
       "42                very good       128\n",
       "43                worthless         9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the total num_seen by parsed:\n",
    "num_seen_to_add = words[words.parsed.duplicated()].groupby('parsed')['num_seen'].agg(np.sum).reset_index()\n",
    "num_seen_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2855, 6)\n"
     ]
    }
   ],
   "source": [
    "# drop the duplicate rows from the dataset\n",
    "words = words[~words.parsed.duplicated()]\n",
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the to-add counts to the data:\n",
    "num_seen_to_add.columns = ['parsed','num_seen_add']\n",
    "words = words.merge(num_seen_to_add, on='parsed', how='left')\n",
    "\n",
    "# fill in null adds with 0:\n",
    "words.loc[words.num_seen_add.isnull(), 'num_seen_add'] = 0\n",
    "\n",
    "# combine num_seen with additions and then remove the num_seen_add columnn\n",
    "words.num_seen = words.num_seen + words.num_seen_add\n",
    "words.drop('num_seen_add', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_seen</th>\n",
       "      <th>message</th>\n",
       "      <th>empathy</th>\n",
       "      <th>parsed</th>\n",
       "      <th>num_words</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>loving my family sad for his relationship</td>\n",
       "      <td>affectionate</td>\n",
       "      <td>loving my family sad for his relationship</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>angry disappointed hopeless</td>\n",
       "      <td>angry</td>\n",
       "      <td>angry disappointed hopeless</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>like i want to scream and beat the shit out of...</td>\n",
       "      <td>angry</td>\n",
       "      <td>like i want to scream and beat the shit out of...</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>im sad and angry</td>\n",
       "      <td>angry</td>\n",
       "      <td>im sad and angry</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i am very angry and very sad</td>\n",
       "      <td>angry</td>\n",
       "      <td>i am very angry and very sad</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_seen                                            message       empathy  \\\n",
       "0       1.0          loving my family sad for his relationship  affectionate   \n",
       "1       1.0                        angry disappointed hopeless         angry   \n",
       "2       1.0  like i want to scream and beat the shit out of...         angry   \n",
       "3       1.0                                   im sad and angry         angry   \n",
       "4       1.0                       i am very angry and very sad         angry   \n",
       "\n",
       "                                              parsed  num_words  polarity  \n",
       "0          loving my family sad for his relationship          7       1.0  \n",
       "1                        angry disappointed hopeless          3      -1.0  \n",
       "2  like i want to scream and beat the shit out of...         12      -1.0  \n",
       "3                                   im sad and angry          4      -1.0  \n",
       "4                       i am very angry and very sad          7      -1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some empathies just don't appear very frequently. I will set a threshold of 5 for what I consider unacceptably infrequent empathies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scared          4\n",
       "motivated       4\n",
       "relief          4\n",
       "bored           4\n",
       "confused        3\n",
       "playful         3\n",
       "sexy            3\n",
       "creative        2\n",
       "stable          2\n",
       "panic           2\n",
       "embarrassed     2\n",
       "ignored         1\n",
       "restless        1\n",
       "affectionate    1\n",
       "lazy            1\n",
       "uneasy          1\n",
       "frustrated      1\n",
       "loved           1\n",
       "intoxicated     1\n",
       "annoyed         1\n",
       "flustered       1\n",
       "Name: empathy, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empathy_counts = words.empathy.value_counts()\n",
    "infrequent = empathy_counts[empathy_counts < 5]\n",
    "infrequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that these empathies don't have huge `num_seen` counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empathy</th>\n",
       "      <th>num_seen</th>\n",
       "      <th>parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>affectionate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>loving my family sad for his relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>annoyed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good but irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>bored</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bored and slightly depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bored</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bored as shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>bored</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bored tired disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>bored</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bored and hopeless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>confused</td>\n",
       "      <td>1.0</td>\n",
       "      <td>im feeling a bit confused kind of down mostly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>confused</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sad and fuzzy and confused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>confused</td>\n",
       "      <td>1.0</td>\n",
       "      <td>confuse but good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>creative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ok i guess been drawing happy with the result ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>creative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>optimistic creative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>embarrassed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i just had boot camp and i have tennis every f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>embarrassed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>embarrased and not good enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>flustered</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good but cluttered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>frustrated</td>\n",
       "      <td>1.0</td>\n",
       "      <td>irritated confused wondering what the fuck is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>ignored</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good but everyone keeps ignoring me today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>intoxicated</td>\n",
       "      <td>2.0</td>\n",
       "      <td>high as fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>lazy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lazy low energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>loved</td>\n",
       "      <td>2.0</td>\n",
       "      <td>loved and happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>motivated</td>\n",
       "      <td>1.0</td>\n",
       "      <td>not bad a little tired moderately motivated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>motivated</td>\n",
       "      <td>1.0</td>\n",
       "      <td>great and feeling motivated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>motivated</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i feel motivated and really happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>motivated</td>\n",
       "      <td>2.0</td>\n",
       "      <td>feeling motivated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>panic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>hopeless  panicked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>panic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>depressed panicked scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>playful</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good goofy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>playful</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good playful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>playful</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good playing games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>relief</td>\n",
       "      <td>1.0</td>\n",
       "      <td>happy relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>relief</td>\n",
       "      <td>1.0</td>\n",
       "      <td>im feeling great reliefed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>relief</td>\n",
       "      <td>1.0</td>\n",
       "      <td>glad thats over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>relief</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sort of happyrelieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>restless</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i feel sad and restless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>scared</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scared angry sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>scared</td>\n",
       "      <td>1.0</td>\n",
       "      <td>not good im scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>scared</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so sad and scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>scared</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scared and hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>sexy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>im feeling active and want to talk to a woman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>sexy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>horny sad erratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>sexy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i want to fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>stable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good steady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>stable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good and steady generally happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>uneasy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>generally good but im always a little on edge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           empathy  num_seen  \\\n",
       "0     affectionate       1.0   \n",
       "7          annoyed       1.0   \n",
       "107          bored       1.0   \n",
       "108          bored       1.0   \n",
       "109          bored       1.0   \n",
       "110          bored       1.0   \n",
       "124       confused       1.0   \n",
       "125       confused       1.0   \n",
       "126       confused       1.0   \n",
       "127       creative       1.0   \n",
       "128       creative       1.0   \n",
       "287    embarrassed       1.0   \n",
       "288    embarrassed       1.0   \n",
       "314      flustered       1.0   \n",
       "315     frustrated       1.0   \n",
       "1860       ignored       1.0   \n",
       "1861   intoxicated       2.0   \n",
       "1862          lazy       1.0   \n",
       "1892         loved       2.0   \n",
       "1893     motivated       1.0   \n",
       "1894     motivated       1.0   \n",
       "1895     motivated       1.0   \n",
       "1896     motivated       2.0   \n",
       "2003         panic       1.0   \n",
       "2004         panic       1.0   \n",
       "2011       playful       1.0   \n",
       "2012       playful       1.0   \n",
       "2013       playful       1.0   \n",
       "2014        relief       1.0   \n",
       "2015        relief       1.0   \n",
       "2016        relief       1.0   \n",
       "2017        relief       1.0   \n",
       "2018      restless       1.0   \n",
       "2657        scared       1.0   \n",
       "2658        scared       1.0   \n",
       "2659        scared       1.0   \n",
       "2660        scared       1.0   \n",
       "2661          sexy       1.0   \n",
       "2662          sexy       1.0   \n",
       "2663          sexy       1.0   \n",
       "2708        stable       1.0   \n",
       "2709        stable       1.0   \n",
       "2801        uneasy       1.0   \n",
       "\n",
       "                                                 parsed  \n",
       "0             loving my family sad for his relationship  \n",
       "7                                    good but irritated  \n",
       "107                        bored and slightly depressed  \n",
       "108                                       bored as shit  \n",
       "109                            bored tired disappointed  \n",
       "110                                  bored and hopeless  \n",
       "124   im feeling a bit confused kind of down mostly ...  \n",
       "125                          sad and fuzzy and confused  \n",
       "126                                    confuse but good  \n",
       "127   ok i guess been drawing happy with the result ...  \n",
       "128                                 optimistic creative  \n",
       "287   i just had boot camp and i have tennis every f...  \n",
       "288                      embarrased and not good enough  \n",
       "314                                  good but cluttered  \n",
       "315   irritated confused wondering what the fuck is ...  \n",
       "1860          good but everyone keeps ignoring me today  \n",
       "1861                                       high as fuck  \n",
       "1862                                    lazy low energy  \n",
       "1892                                    loved and happy  \n",
       "1893        not bad a little tired moderately motivated  \n",
       "1894                        great and feeling motivated  \n",
       "1895                  i feel motivated and really happy  \n",
       "1896                                  feeling motivated  \n",
       "2003                                 hopeless  panicked  \n",
       "2004                          depressed panicked scared  \n",
       "2011                                         good goofy  \n",
       "2012                                       good playful  \n",
       "2013                                 good playing games  \n",
       "2014                                       happy relief  \n",
       "2015                          im feeling great reliefed  \n",
       "2016                                    glad thats over  \n",
       "2017                              sort of happyrelieved  \n",
       "2018                            i feel sad and restless  \n",
       "2657                                   scared angry sad  \n",
       "2658                                 not good im scared  \n",
       "2659                                  so sad and scared  \n",
       "2660                                    scared and hurt  \n",
       "2661  im feeling active and want to talk to a woman ...  \n",
       "2662                                  horny sad erratic  \n",
       "2663                                     i want to fuck  \n",
       "2708                                        good steady  \n",
       "2709                    good and steady generally happy  \n",
       "2801      generally good but im always a little on edge  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see, they have low (1 or 2) counts, and so we can safely remove them.\n",
    "words.loc[words.empathy.isin(infrequent.index.values), ['empathy','num_seen','parsed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the infrequent empathies from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2812, 6)\n"
     ]
    }
   ],
   "source": [
    "words = words[~words.empathy.isin(infrequent.index.values)]\n",
    "words.reset_index(drop=True, inplace=True)\n",
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to code the empathies as integer labels to be input to the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empathy_le = LabelEncoder()\n",
    "empathy_le.fit(words.empathy.unique())\n",
    "words['label'] = empathy_le.transform(words.empathy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the parsed messages as list of list character indices. We can remove the space character since this network takes whole words at once. \n",
    "\n",
    "End-of-word and end-of-sentence indices are added by the `char_to_inds` function.\n",
    "\n",
    "> Note: I have changed the format/behavior of the indices slightly from when I presented. Before there was a start-of-sentence and end-of-sentence placeholder. I think the start-of-sentence doesn't have a lot of value and also thought that end-of-word was missing from the original setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_seen</th>\n",
       "      <th>message</th>\n",
       "      <th>empathy</th>\n",
       "      <th>parsed</th>\n",
       "      <th>num_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>label</th>\n",
       "      <th>char_inds</th>\n",
       "      <th>num_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>angry disappointed hopeless</td>\n",
       "      <td>angry</td>\n",
       "      <td>angry disappointed hopeless</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[3, 16, 9, 20, 27, 1], [6, 11, 21, 3, 18, 18,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>like i want to scream and beat the shit out of...</td>\n",
       "      <td>angry</td>\n",
       "      <td>like i want to scream and beat the shit out of...</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[14, 11, 13, 7, 1], [11, 1], [25, 3, 16, 22, ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>im sad and angry</td>\n",
       "      <td>angry</td>\n",
       "      <td>im sad and angry</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[11, 15, 1], [21, 3, 6, 1], [3, 16, 6, 1], [3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i am very angry and very sad</td>\n",
       "      <td>angry</td>\n",
       "      <td>i am very angry and very sad</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[11, 1], [3, 15, 1], [24, 7, 20, 27, 1], [3, ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>jealous and mad</td>\n",
       "      <td>angry</td>\n",
       "      <td>jealous and mad</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[12, 7, 3, 14, 17, 23, 21, 1], [3, 16, 6, 1],...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_seen                                            message empathy  \\\n",
       "0       1.0                        angry disappointed hopeless   angry   \n",
       "1       1.0  like i want to scream and beat the shit out of...   angry   \n",
       "2       1.0                                   im sad and angry   angry   \n",
       "3       1.0                       i am very angry and very sad   angry   \n",
       "4       1.0                                    jealous and mad   angry   \n",
       "\n",
       "                                              parsed  num_words  polarity  \\\n",
       "0                        angry disappointed hopeless          3      -1.0   \n",
       "1  like i want to scream and beat the shit out of...         12      -1.0   \n",
       "2                                   im sad and angry          4      -1.0   \n",
       "3                       i am very angry and very sad          7      -1.0   \n",
       "4                                    jealous and mad          3      -1.0   \n",
       "\n",
       "   label                                          char_inds  num_chars  \n",
       "0      0  [[3, 16, 9, 20, 27, 1], [6, 11, 21, 3, 18, 18,...          4  \n",
       "1      0  [[14, 11, 13, 7, 1], [11, 1], [25, 3, 16, 22, ...         13  \n",
       "2      0  [[11, 15, 1], [21, 3, 6, 1], [3, 16, 6, 1], [3...          5  \n",
       "3      0  [[11, 1], [3, 15, 1], [24, 7, 20, 27, 1], [3, ...          8  \n",
       "4      0  [[12, 7, 3, 14, 17, 23, 21, 1], [3, 16, 6, 1],...          4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique chars and remove the space (will be lowercase alphabet):\n",
    "all_chars = [x for x in np.unique(list(' '.join(list(words.parsed.values)))) if not x == ' ']\n",
    "chars_size = len(all_chars)+3\n",
    "\n",
    "# fit the character to index label encoder:\n",
    "char_le = LabelEncoder()\n",
    "char_le.fit(all_chars)\n",
    "\n",
    "# set up placeholder indices: one for unknown, one for end of word, one for end of sentence:\n",
    "ZERO_IND = 0\n",
    "EOW_IND = 1\n",
    "EOS_IND = 2\n",
    "\n",
    "# convert the parsed messages into index lists:\n",
    "words['char_inds'] = words.parsed.map(lambda x: char_to_inds(x, char_le, eow_ind=EOW_IND, eos_ind=EOS_IND))\n",
    "words['num_chars'] = words.char_inds.map(lambda x: len(x))\n",
    "\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create observation weights according to `num_words`\n",
    "---\n",
    "\n",
    "Some messages are written in a huge amount of times, and we want the model to do well on those. Rather than upsampling now, I can create a weighting vector for the observations. For now it will be the frequencies by phrase stored in `num_seen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_weights = words.num_seen.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the upsampled data into training and testing, with 80/20 split. Also split the observation weights along the same indices, then inver the training observation frequencies and convert to probability of selection.\n",
    "\n",
    "**Using this approach the training and testing set will have completely unique messages!** That may be desirable or undesirable to you, depending on your perspective. It's certainly a more difficult problem for the network to solve. The alternative is to first upsample the dataset according to `num_words` and then split after that. Then the repeated message are likely to appear in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2249, 9) (2249,) (563, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, testing, train_obs_weights, _ = train_test_split(words, obs_weights, test_size=0.2, \n",
    "                                                           stratify=words.label.values,\n",
    "                                                           random_state=779)\n",
    "print(training.shape, train_obs_weights.shape, testing.shape)\n",
    "\n",
    "train_obs_weights = 1./train_obs_weights\n",
    "train_obs_weights = train_obs_weights/np.sum(train_obs_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset down the training and testing to just columns we want, and then create a `class_weight` vector based on the occurances of the empathy targets. \n",
    "\n",
    "The weighting for a class (which is applied during the calculation of the loss) is it's inverse frequency in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_weight</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.857143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.120930</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.590909</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.714286</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.600000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.008710</td>\n",
       "      <td>2985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.130435</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.288889</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.490566</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.012994</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.888889</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_weight  frequency\n",
       "0       5.200000        5.0\n",
       "1       6.500000        4.0\n",
       "2       0.400000       65.0\n",
       "3       1.857143       14.0\n",
       "4       4.333333        6.0\n",
       "5       6.500000        4.0\n",
       "6       0.120930      215.0\n",
       "7       0.590909       44.0\n",
       "8       0.650000       40.0\n",
       "9       3.714286        7.0\n",
       "10      2.600000       10.0\n",
       "11      6.500000        4.0\n",
       "12      0.008710     2985.0\n",
       "13      4.333333        6.0\n",
       "14      1.130435       23.0\n",
       "15      0.288889       90.0\n",
       "16      0.490566       53.0\n",
       "17      5.200000        5.0\n",
       "18      0.012994     2001.0\n",
       "19      0.787879       33.0\n",
       "20      4.333333        6.0\n",
       "21      2.888889        9.0\n",
       "22      0.433333       60.0\n",
       "23      4.333333        6.0\n",
       "24      0.684211       38.0\n",
       "25      0.702703       37.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = training[['num_seen','empathy','parsed','label','char_inds']]\n",
    "testing = testing[['num_seen','empathy','parsed','label','char_inds']]\n",
    "\n",
    "class_weights = 1./training.groupby('label')['num_seen'].agg(np.sum).sort_index()\n",
    "class_weights = class_weights.values*class_weights.shape[0]\n",
    "\n",
    "\n",
    "pd.DataFrame({'frequency':training.groupby('label')['num_seen'].agg(np.sum).sort_index().values,\n",
    "              'class_weight':class_weights},\n",
    "             index=training['label'].value_counts().sort_index().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network\n",
    "---\n",
    "\n",
    "Below we set up the hierarchical network with the parameters for layer sizes, dropout amounts, and some other options.\n",
    "\n",
    "    char_emb_size = 16\n",
    "        This is the dimensionality of the embedding vector for characters. This means\n",
    "        that a given character, for example 'f', is represented by a vector of 16 numbers.\n",
    "        This vector representation is trainable by the network.\n",
    "    num_empathies = 26\n",
    "        The count of the target empathy classes. This will be the size of the final output\n",
    "        of the network - the class likelihoods as predicted by the message representation.\n",
    "    word_hidden_size = 50\n",
    "        The word vector representation size. Because the RNN is bidirectional this vector\n",
    "        will internally actually be 100 units.\n",
    "    message_hidden_size = 50\n",
    "        The message vector representation. Again, like the words, this will turn out to be\n",
    "        100 due to the bidirectional RNN. So, your output message representations will be\n",
    "        100 dimensions per message.\n",
    "    word_context_size = 50\n",
    "        This is the size of the trainable \"attention\" layer in the CharToWord section of the\n",
    "        network.\n",
    "    message_context_size = 50\n",
    "        This is the size of the trainable \"attention\" layer in the WordToMessage section.\n",
    "    intermediate_nonlinear_layer = True\n",
    "        This adds a layer between the message and output prediction layer with a nonlinear\n",
    "        activation function. Note that you will lose the ability to say that the output \n",
    "        likelihoods/probabilities are a linear transformation of the message vectors!\n",
    "        \n",
    "I have also set the \"dropout\" probabilities for word, message, and output to be 0.2. These dropouts are applied at the representation level only. What this means is that there is a 20% chance that outgoing weights from a representation will be \"dropped\" or missing. Dropout is a good way to regularize neural networks (though can be tricky with RNNs) and is the conceptual equivalent to ensembling of models.\n",
    "\n",
    "> Note: The model I presented had `intermediate_nonlinear_layer = False` because I wanted the likelihoods of classes to be a linear transformation of the message representations. I've set it to `True` just to demonstrate in the model below. It'd definitely be worth investigating how the message vectors differ between the two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_emb_size = 16\n",
    "num_empathies = len(words['label'].unique())\n",
    "word_hidden_size = 50\n",
    "message_hidden_size = 50\n",
    "word_context_size = 50\n",
    "message_context_size = 50\n",
    "intermediate_nonlinear_layer = True\n",
    "\n",
    "gru = HierarchicalAttentionRNN(chars_size, char_emb_size, word_hidden_size, message_hidden_size, \n",
    "                               word_context_size, message_context_size, num_empathies,\n",
    "                               word_dropout_p=0.2, message_dropout_p=0.2, output_dropout_p=0.2,\n",
    "                               char_projection_nonlinearity=nn.ELU, \n",
    "                               word_projection_nonlinearity=nn.ELU,\n",
    "                               intermediate_output_step=intermediate_nonlinear_layer,\n",
    "                               char_to_word_rnn=nn.GRU, word_to_message_rnn=nn.GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the save files. One is saved upon completion, the other is saved when the network achieves its best performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file = './test_params.pt'\n",
    "save_best = './test_params_best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out the required numpy arrays from the pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_targets = training['label'].values\n",
    "train_char_indices = training['char_inds'].values\n",
    "\n",
    "test_targets = testing['label'].values\n",
    "test_char_indices = testing['char_inds'].values\n",
    "test_messages = testing['parsed'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary mapping the labels to the empathies is required by the `Evaluation` class. (This class is used for testing and printing out examples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_ref_dict = words[['label','empathy']].drop_duplicates()\n",
    "target_ref_dict = {k:v for k,v in zip(target_ref_dict.label.values,\n",
    "                                      target_ref_dict.empathy.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Evaluation` object requires:\n",
    "    \n",
    "    test_char_indices\n",
    "        these are the list-of-list indices from the test set\n",
    "    test_targets\n",
    "        these are the empathy labels from the test set\n",
    "    test_messages\n",
    "        the actual message text corresponding to the character indices\n",
    "    target_ref_dict\n",
    "        a label-to-empathy mapping dictionary\n",
    "    char_le\n",
    "        the label encoder for the characters\n",
    "        \n",
    "This object performs testing, demoing, and printing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = Evaluation(test_char_indices, test_targets, test_messages, target_ref_dict, char_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy loss function penalty\n",
    "---\n",
    "\n",
    "OK before going forward it is important to cover the entropy penalty I added to the loss function of the neural network. It is not something that was within the scope of my presentation but it has an important regularizing effect on the network's outputs (if you choose to use it!).\n",
    "\n",
    "Entropy is an old concept from information theory. It is a metric that measures the expected information from a probability distribution of possible events. When outcomes are certain, entropy is zero. As probability of events becomes more uniform, entropy increases.\n",
    "\n",
    "You can calculate the entropy of a distribution of outcomes as:\n",
    "\n",
    "### $$ H(X) = - \\sum_{i=1}^n P(x_i)\\;ln(P(x_i)) $$\n",
    "\n",
    "As a relevant example, let's take the predicted probabilities empathy class membership for a given message. Below I'll set up two fake probability distributions across empathies. You can see that the entropy for the distribution where an outcome is much more likely relative to the others has a lower empathy than the more uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def entropy(p_vector):\n",
    "    return -1 * np.sum(p_vector * np.log(p_vector + 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAByAAAANYCAYAAACM2PpGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e7b7978>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 428,
       "width": 912
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_labels = np.array(sorted(target_ref_dict.keys()))\n",
    "\n",
    "# set up a mesage with high certainty about one empathy:\n",
    "certain = np.array([1. if e == 9 else 0. for e in emp_labels]) + np.random.random(len(emp_labels))*0.02\n",
    "certain = certain/certain.sum()\n",
    "\n",
    "# set up an uncertain message - many empathies are possible:\n",
    "uncertain = stats.norm(loc=20, scale=10).pdf(emp_labels) + np.random.random(len(emp_labels))*0.05\n",
    "uncertain = uncertain/uncertain.sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,7))\n",
    "sns.barplot(emp_labels, certain, color='darkred', ci=None, ax=ax[0])\n",
    "sns.barplot(emp_labels, uncertain, color='steelblue', ci=None, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('\"Certain\" Message Entropy = %.4f' % (entropy(certain)))\n",
    "ax[1].set_title('\"Uncertain\" Message Entropy = %.4f' % (entropy(uncertain)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard loss for this classifier neural network is the cross entropy loss. Imagine we have our message's predicted probability distribution over empathies, denoted $p$. We also have the actual occuring empathy for that message (the target) as $y$, which will be a 1 at the correct empathy and zero everywhere else. The cross entropy is:\n",
    "\n",
    "### $$ H(y, p) = - \\sum_{i=1}^n y_i \\; ln(p_i) $$\n",
    "\n",
    "Minimizing the cross-entropy is equivalent to minimizing the negative log-likelihood.\n",
    "\n",
    "We can take a look at what our cross entropy losses would be for these messages, given two target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss for certain: 0.237592169105\n",
      "CE Loss for uncertain: 3.00385007144\n",
      "Difference in loss, uncertain-certain: 2.76625790234\n"
     ]
    }
   ],
   "source": [
    "certain_target = np.array([1. if e == 9 else 0. for e in emp_labels])\n",
    "uncertain_target = np.array([1. if e == 20 else 0. for e in emp_labels])\n",
    "\n",
    "def cross_entropy(true, pred_prob):\n",
    "    return -1 * np.sum(true * np.log(pred_prob + 1e-12))\n",
    "\n",
    "certain_ce = cross_entropy(certain_target, certain)\n",
    "uncertain_ce = cross_entropy(uncertain_target, uncertain)\n",
    "\n",
    "print('CE Loss for certain:', certain_ce)\n",
    "print('CE Loss for uncertain:', uncertain_ce)\n",
    "print('Difference in loss, uncertain-certain:', uncertain_ce-certain_ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, obviously our \"certain\" vector of predicted probabilities has a lower loss since the target class aligns with its prediction.\n",
    "\n",
    "But maybe we want to avoid getting _really_ certain about target classes. Having super high certainty about classes is indicative of overfitting. We can actually add a penalty to the loss function that rewards predicted probability distributions with higher entropy:\n",
    "\n",
    "### $$ L(y, p) = - \\sum_{i=1}^n y_i \\; ln(p_i) - \\beta \\sum_{i=1}^n p_i\\;ln(p_i) $$\n",
    "\n",
    "Where $\\beta$ is some regularization strength.\n",
    "\n",
    "We can set $\\beta = 1.0$ and see what happens to our losses when we apply the penalty favoring higher entropy distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalized loss for certain: -0.918324290661\n",
      "Penalized loss for uncertain: -0.205721413891\n",
      "Difference in loss, uncertain-certain: 0.71260287677\n"
     ]
    }
   ],
   "source": [
    "beta = 1.0\n",
    "def penalized_loss(true, pred_prob, beta):\n",
    "    return cross_entropy(true, pred_prob) - (beta * entropy(pred_prob))\n",
    "\n",
    "certain_loss = penalized_loss(certain_target, certain, beta)\n",
    "uncertain_loss = penalized_loss(uncertain_target, uncertain, beta)\n",
    "\n",
    "print('Penalized loss for certain:', certain_loss)\n",
    "print('Penalized loss for uncertain:', uncertain_loss)\n",
    "print('Difference in loss, uncertain-certain:', uncertain_loss-certain_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss can now go below zero, but that's fine, we are just trying to achieve any minimum. You can see that the difference between losses has shrunk considerably since the certain vector is being \"punished\" more than the uncertain one for having a lower entropy of predicted probabilities.\n",
    "\n",
    "The choice of $\\beta$ sets the importance of having a high-entropy distribution over predicted classes. You could of course turn it up so high that the certain vector was actually performing _worse_ than the uncertain one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalized loss for certain: -11.3215724286\n",
      "Penalized loss for uncertain: -29.0918647819\n",
      "Difference in loss, uncertain-certain: -17.7702923534\n"
     ]
    }
   ],
   "source": [
    "extreme_beta = 10.\n",
    "certain_loss = penalized_loss(certain_target, certain, extreme_beta)\n",
    "uncertain_loss = penalized_loss(uncertain_target, uncertain, extreme_beta)\n",
    "\n",
    "print('Penalized loss for certain:', certain_loss)\n",
    "print('Penalized loss for uncertain:', uncertain_loss)\n",
    "print('Difference in loss, uncertain-certain:', uncertain_loss-certain_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-adjusting $\\beta$\n",
    "\n",
    "Rather than choose an explicit, set value for the regularization strength, I allowed it to adjust itself as a function of the training accuracy like so:\n",
    "\n",
    "### $$ \\beta = \\begin{cases}\n",
    "\\text{tanh}(acc) & \\text{if} ~~ acc \\lt 0.99 \\\\\n",
    "\\text{tanh}(0.99) & \\text{if} ~~ acc \\ge 0.99\n",
    "\\end{cases} $$\n",
    "\n",
    "When the training accuracy is low, the regularization strength will be low. As the training accuracy increases, so does the regularization strength. This is equivalent to telling the network, \"as you improve at identifying the empathies correctly, increase the value of entropy in predicted probabilies.\"\n",
    "\n",
    "This way, the network focuses more on generalization ability as it improves on the training set, but when it is terrible on the training data it focuses effort on the standard cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "---\n",
    "\n",
    "The `Trainer` object will control the fitting and iterations of the model. There are a variety of options you can set to control the behavior of the fitting procedure.\n",
    "\n",
    "There are only three required arguments:\n",
    "\n",
    "    model\n",
    "    train_char_indices\n",
    "    train_targets\n",
    "    \n",
    "The optional arguments are:\n",
    "\n",
    "    class_weights (default None)\n",
    "        determines how to weight the loss function per class (to deal with class imbalance)\n",
    "    verbose (default True)\n",
    "        print out progress statements or not\n",
    "    save_path (default None)\n",
    "        where to save the final model parameters\n",
    "    save_on_path_best (default None)\n",
    "        where to save the best model on testing\n",
    "    evaluator (default None)\n",
    "        an Evaluator object - if not supplied then no test-set validation is done\n",
    "    sample_size (default 1000)\n",
    "        the sample size of training sample per iteration\n",
    "    with_replacement (default True)\n",
    "        boolean indicating whether to sample the training data with replacement or not\n",
    "    sample_p (default None)\n",
    "        sample weights for training data, uniform sampling if left blank\n",
    "    early_stopping_test_acc (default None)\n",
    "        threshold for test accuracy in which to stop model fitting\n",
    "    learning_rate (default 0.01)\n",
    "        how fast should descent on the gradient move\n",
    "    entropy_weight (default 0.0)\n",
    "        how much to weight the class probability entropy penalty of the loss function\n",
    "    weight_entropy_by_accuracy (default False)\n",
    "        this will increase or decrease the importance of the entropy penalty as a function of\n",
    "        training accuracy (if false, entropy_weight is the regularization strength)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run until a testing accuracy of 0.6 is achieved or 5 iterations pass, then stop.**\n",
    "\n",
    "You'll obviously want to run it for more than 5 iterations to get good results... but for now just to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = Trainer(gru, train_char_indices, train_targets, class_weights=class_weights,\n",
    "            verbose=True, save_path=save_file, save_on_best_path=save_best,\n",
    "            evaluator=E, sample_size=1000, sample_p=train_obs_weights,\n",
    "            entropy_weight=1.0, weight_entropy_by_accuracy=True,\n",
    "            early_stopping_test_acc=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "New entropy weight: 0.0220\n",
      "MODEL: 0m 46s (- 38m 14s) (1 2%)   loss: 0.3615   acc: 0.0220\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.4327   acc: 0.0071\n",
      "-----------------------------------------------------\n",
      "actually kinda good\n",
      "   label         p empathy\n",
      "0     12  0.035938   great\n",
      "Predicted:\n",
      "   label         p     empathy\n",
      "0     14  0.081871        loss\n",
      "1     20  0.071697       sleep\n",
      "2      2  0.064635     average\n",
      "3      5  0.063750  confidence\n",
      "4     17  0.061940     pensive\n",
      "-----------------------------------------------------\n",
      "Iteration: 2\n",
      "New entropy weight: 0.0150\n",
      "MODEL: 1m 37s (- 39m 9s) (2 4%)   loss: 0.5345   acc: 0.0150\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.5269   acc: 0.0053\n",
      "-----------------------------------------------------\n",
      "great my shop is also going great\n",
      "   label         p empathy\n",
      "0     12  0.028822   great\n",
      "Predicted:\n",
      "   label         p      empathy\n",
      "0      0  0.181728        angry\n",
      "1      9  0.101998    emotional\n",
      "2      3  0.092387       better\n",
      "3     10  0.078426  emotionless\n",
      "4     22  0.047379        tired\n",
      "-----------------------------------------------------\n",
      "Iteration: 3\n",
      "New entropy weight: 0.0140\n",
      "MODEL: 2m 28s (- 38m 49s) (3 6%)   loss: 0.6916   acc: 0.0140\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.4969   acc: 0.0053\n",
      "-----------------------------------------------------\n",
      "not terrible trying to get out if this rut\n",
      "   label         p empathy\n",
      "0     15  0.030231    okay\n",
      "Predicted:\n",
      "   label         p      empathy\n",
      "0     11  0.191947      excited\n",
      "1      9  0.135116    emotional\n",
      "2     24  0.060789  unmotivated\n",
      "3     20  0.054634        sleep\n",
      "4     10  0.052802  emotionless\n",
      "-----------------------------------------------------\n",
      "Iteration: 4\n",
      "New entropy weight: 0.0080\n",
      "MODEL: 3m 25s (- 39m 26s) (4 8%)   loss: 0.3481   acc: 0.0080\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.4054   acc: 0.0124\n",
      "-----------------------------------------------------\n",
      "feeling good had a good day\n",
      "   label         p empathy\n",
      "0     12  0.028479   great\n",
      "Predicted:\n",
      "   label         p     empathy\n",
      "0     23  0.079936   uncertain\n",
      "1      7  0.077463  determined\n",
      "2      1  0.068939     anxious\n",
      "3      9  0.066982   emotional\n",
      "4     21  0.062902    stressed\n",
      "-----------------------------------------------------\n",
      "Iteration: 5\n",
      "New entropy weight: 0.0140\n",
      "MODEL: 4m 18s (- 38m 49s) (5 10%)   loss: 0.2534   acc: 0.0140\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.4896   acc: 0.0213\n",
      "-----------------------------------------------------\n",
      "there are two amazing things in the world and i want both of them\n",
      "   label        p    empathy\n",
      "0     23  0.05224  uncertain\n",
      "Predicted:\n",
      "   label         p      empathy\n",
      "0     24  0.129405  unmotivated\n",
      "1     21  0.092860     stressed\n",
      "2     14  0.091320         loss\n",
      "3     13  0.060368        happy\n",
      "4      9  0.053364    emotional\n",
      "-----------------------------------------------------\n",
      "Iteration: 6\n",
      "New entropy weight: 0.0220\n",
      "MODEL: 5m 10s (- 37m 54s) (6 12%)   loss: -0.0465   acc: 0.0220\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.5296   acc: 0.0231\n",
      "-----------------------------------------------------\n",
      "not terrible trying to get out if this rut\n",
      "   label         p empathy\n",
      "0     15  0.042935    okay\n",
      "Predicted:\n",
      "   label         p      empathy\n",
      "0     24  0.101572  unmotivated\n",
      "1      5  0.093038   confidence\n",
      "2      1  0.085936      anxious\n",
      "3     10  0.075779  emotionless\n",
      "4     20  0.075057        sleep\n",
      "-----------------------------------------------------\n",
      "Iteration: 7\n",
      "New entropy weight: 0.0190\n",
      "MODEL: 5m 58s (- 36m 40s) (7 14%)   loss: 0.1600   acc: 0.0190\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.4192   acc: 0.0266\n",
      "-----------------------------------------------------\n",
      "good happy\n",
      "   label         p empathy\n",
      "0     12  0.040856   great\n",
      "Predicted:\n",
      "   label         p     empathy\n",
      "0      1  0.064019     anxious\n",
      "1      7  0.058866  determined\n",
      "2     17  0.056320     pensive\n",
      "3      5  0.052666  confidence\n",
      "4     15  0.048532        okay\n",
      "-----------------------------------------------------\n",
      "Iteration: 8\n",
      "New entropy weight: 0.0310\n",
      "MODEL: 6m 46s (- 35m 32s) (8 16%)   loss: -0.1784   acc: 0.0310\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.3009   acc: 0.0337\n",
      "-----------------------------------------------------\n",
      "not great but ok\n",
      "   label         p empathy\n",
      "0     15  0.032359    okay\n",
      "Predicted:\n",
      "   label         p      empathy\n",
      "0     10  0.071047  emotionless\n",
      "1     24  0.067345  unmotivated\n",
      "2     20  0.062342        sleep\n",
      "3     22  0.056408        tired\n",
      "4     17  0.052638      pensive\n",
      "-----------------------------------------------------\n",
      "Iteration: 9\n",
      "New entropy weight: 0.0370\n",
      "MODEL: 7m 41s (- 35m 1s) (9 18%)   loss: -0.1080   acc: 0.0370\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.3234   acc: 0.0337\n",
      "-----------------------------------------------------\n",
      "wired good\n",
      "   label         p empathy\n",
      "0     12  0.046961   great\n",
      "Predicted:\n",
      "   label         p     empathy\n",
      "0     22  0.060754       tired\n",
      "1     20  0.058504       sleep\n",
      "2      7  0.053599  determined\n",
      "3     17  0.048645     pensive\n",
      "4     12  0.046961       great\n",
      "-----------------------------------------------------\n",
      "Iteration: 10\n",
      "New entropy weight: 0.0500\n",
      "MODEL: 8m 30s (- 34m 2s) (10 20%)   loss: -0.2918   acc: 0.0500\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.3874   acc: 0.0302\n",
      "-----------------------------------------------------\n",
      "pretty good ate good food\n",
      "   label         p empathy\n",
      "0     12  0.038119   great\n",
      "Predicted:\n",
      "   label         p     empathy\n",
      "0      9  0.085979   emotional\n",
      "1      2  0.073158     average\n",
      "2      5  0.070295  confidence\n",
      "3     23  0.061884   uncertain\n",
      "4     20  0.059597       sleep\n",
      "-----------------------------------------------------\n",
      "Iteration: 11\n",
      "New entropy weight: 0.0350\n",
      "MODEL: 9m 28s (- 33m 35s) (11 22%)   loss: -0.2296   acc: 0.0350\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.4033   acc: 0.0266\n",
      "-----------------------------------------------------\n",
      "just finished work feeling exhausted and tired and hurt\n",
      "   label        p empathy\n",
      "0     22  0.12668   tired\n",
      "Predicted:\n",
      "   label         p     empathy\n",
      "0      3  0.184673      better\n",
      "1     22  0.126680       tired\n",
      "2     21  0.110798    stressed\n",
      "3      0  0.057692       angry\n",
      "4      8  0.048541  distortion\n",
      "-----------------------------------------------------\n",
      "Iteration: 12\n",
      "New entropy weight: 0.0340\n",
      "MODEL: 10m 23s (- 32m 55s) (12 24%)   loss: -0.2695   acc: 0.0340\n",
      "-----------------------------------------------------\n",
      "TESTING:   loss: 3.3993   acc: 0.0142\n",
      "-----------------------------------------------------\n",
      "bot good at all\n",
      "   label         p empathy\n",
      "0     12  0.032285   great\n",
      "Predicted:\n",
      "   label         p      empathy\n",
      "0      9  0.091764    emotional\n",
      "1     20  0.062683        sleep\n",
      "2      2  0.061566      average\n",
      "3     23  0.061518    uncertain\n",
      "4     24  0.054014  unmotivated\n",
      "-----------------------------------------------------\n",
      "Iteration: 13\n"
     ]
    }
   ],
   "source": [
    "T.training_loop(n_iters=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluating on new phrases\n",
    "---\n",
    "\n",
    "You can use the instantiated `Evaluation` object to get predicted probabilities of empathies for your own phrases. You can test for example if the model is resilient to spelling errors.\n",
    "\n",
    "(You'll need to actually train it long enough to get reasonable output!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel great\n",
      "   label         p  empathy\n",
      "0     19  0.054955     sick\n",
      "1     20  0.054923    sleep\n",
      "2     17  0.053169  pensive\n",
      "3     11  0.047842  excited\n",
      "4     22  0.047028    tired\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "E.eval_message(gru, 'i feel great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ix ffeel grreaat\n",
      "   label         p     empathy\n",
      "0     19  0.062890        sick\n",
      "1     17  0.057976     pensive\n",
      "2     22  0.049576       tired\n",
      "3      6  0.048302   depressed\n",
      "4      7  0.045649  determined\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "E.eval_message(gru, 'ix ffeel grreaat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
